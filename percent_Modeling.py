# %%
import pandas as pd
import numpy as np
from scipy.optimize import minimize
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import combinations

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['figure.dpi'] = 100

# ===================== ç»Ÿä¸€æ–‡ä»¶å‘½åå¸¸é‡åŒº =====================
# ã€è¾“å…¥æ–‡ä»¶ã€‘
INPUT_PERCENT_MODELING_DATA = '2_PERCENT_modeling_data.csv'  # ç™¾åˆ†æ¯”å»ºæ¨¡åŸºç¡€æ•°æ®
# ã€åˆ†æç»“æœè¾“å‡ºã€‘
OUTPUT_PERCENT_UNCERTAINTY_ANALYSIS = 'PERCENT_vote_uncertainty_analysis.csv'
OUTPUT_PERCENT_SOLUTION_SPACE = 'PERCENT_solution_space.csv'
# ã€é¢„æµ‹ç»“æœè¾“å‡ºã€‘
OUTPUT_PERCENT_PREDICTED_VOTES = '3_PERCENT_predicted_fan_votes.csv'
# ã€å¯è§†åŒ–æ–‡ä»¶ã€‘
OUTPUT_PERCENT_VISUALIZATION = 'PERCENT_uncertainty_analysis.png'
# ============================================================

# %%
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å¯¼å…¥å’Œé¢„å¤„ç†
print("=== ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å¯¼å…¥å’Œé¢„å¤„ç† ===")
df = pd.read_csv(INPUT_PERCENT_MODELING_DATA)
print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")
print(f"åˆ—å: {list(df.columns)}")
print(f"èµ›å­£åˆ†å¸ƒ: {sorted(df['season'].unique())}")

# æ•°æ®è´¨é‡æ£€æŸ¥
print("\næ•°æ®è´¨é‡æ£€æŸ¥:")
print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:")
print(df.isnull().sum())
print(f"æ•°æ®ç±»å‹:")
print(df.dtypes)

# åˆ›å»ºå‘¨æ¬¡åˆ†ç»„æ ‡è¯†
df['season_week'] = df['season'].astype(str) + '_' + df['week'].astype(str)

# å¦‚æœæ²¡æœ‰æ·˜æ±°æ ‡è®°ï¼ŒåŸºäºâ€œæœ€åä¸€å‘¨â€æ„é€  eliminated_this_week
if 'eliminated_this_week' not in df.columns:
    df = df.sort_values(['celebrity_name', 'season', 'week']).copy()
    df['next_week'] = df.groupby(['celebrity_name', 'season'])['week'].shift(-1)
    df['eliminated_this_week'] = df['next_week'].isna()
    if 'placement' in df.columns:
        df.loc[df['placement'] == 1, 'eliminated_this_week'] = False
    df = df.drop(columns=['next_week'])
    print("å·²è‡ªåŠ¨ç”Ÿæˆ eliminated_this_week åˆ—")

print("\nç¬¬ä¸€éƒ¨åˆ†å®Œæˆ! æ•°æ®å·²æˆåŠŸå¯¼å…¥å¹¶é¢„å¤„ç†ã€‚")

# %%
# ç¬¬äºŒéƒ¨åˆ†ï¼šç™¾åˆ†æ¯”ç¥¨æ•°åˆ†é…å‡½æ•°
print("\n=== ç¬¬äºŒéƒ¨åˆ†ï¼šç™¾åˆ†æ¯”ç¥¨æ•°åˆ†é…å‡½æ•° ===")

# å®å®šä¹‰å…¬å…±å¸¸é‡ï¼ˆç»Ÿä¸€é»˜è®¤å‚æ•°ï¼‰
N_SAMPLES = 1000       # è’™ç‰¹å¡æ´›é‡‡æ ·æ¬¡æ•°ï¼ˆæµ‹è¯•é˜¶æ®µç¼©å°ï¼‰
TOTAL_VOTES = 1000000 # æ€»æŠ•ç¥¨æ•°
CI_LOW = 2.5          # 95%ç½®ä¿¡åŒºé—´ä¸‹é™åˆ†ä½æ•°
CI_HIGH = 97.5        # 95%ç½®ä¿¡åŒºé—´ä¸Šé™åˆ†ä½æ•°
NOISE_STD = 0.6       # é‡‡æ ·æ‰°åŠ¨å¼ºåº¦ï¼ˆä¸Rankæ¨¡å‹ä¸€è‡´çš„æ‰°åŠ¨æ€æƒ³ï¼‰

def normalize_percentages(arr):
    """å°†ç™¾åˆ†æ¯”è£å‰ªå¹¶å½’ä¸€åŒ–åˆ°æ€»å’Œ100"""
    arr = np.clip(np.array(arr), 0, None)
    total = arr.sum()
    if total <= 0:
        return np.ones(len(arr)) / len(arr) * 100
    return arr / total * 100

def percentage_to_votes(percentages, total_votes=TOTAL_VOTES):
    """
    å°†ç™¾åˆ†æ¯”è½¬æ¢ä¸ºç¥¨æ•°ï¼šç™¾åˆ†æ¯”è¶Šé«˜ï¼Œç¥¨æ•°è¶Šå¤š
    ä½¿ç”¨ç»„åˆæƒé‡ç¡®ä¿åˆç†åˆ†å¸ƒ
    """
    percentages = np.array(percentages)
    
    # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨ç™¾åˆ†æ¯”ä½œä¸ºæƒé‡åŸºç¡€
    weights_direct = percentages
    
    # æ–¹æ³•2: å¹³æ–¹æ ¹å¹³æ»‘ - å‡å°‘æç«¯å·®å¼‚
    weights_sqrt = np.sqrt(percentages)
    
    # æ–¹æ³•3: å¯¹æ•°å¹³æ»‘ - è¿›ä¸€æ­¥å¹³æ»‘å·®å¼‚
    weights_log = np.log(percentages + 1)
    
    # ç»„åˆå¤šç§æƒé‡æ–¹æ¡ˆï¼Œç¡®ä¿åˆ†é…åˆç†
    combined_weights = (weights_direct * 0.6 +
                       weights_sqrt * 0.25 +
                       weights_log * 0.15)
    
    # å½’ä¸€åŒ–æƒé‡
    total_weight = np.sum(combined_weights)
    if total_weight > 0:
        normalized_weights = combined_weights / total_weight
    else:
        # ä¿åº•æ–¹æ¡ˆï¼šå‡åŒ€åˆ†é…
        normalized_weights = np.ones(len(percentages)) / len(percentages)
    
    # è®¡ç®—ç¥¨æ•°
    votes = total_votes * normalized_weights
    return votes

def estimate_fan_percentages_heuristic(weekly_data, noise_std=NOISE_STD):
    """
    å¯å‘å¼æ–¹æ³•ï¼šå½“ä¼˜åŒ–æ–¹æ³•å¤±è´¥æ—¶ä½¿ç”¨
    åŸºäºè¯„å§”ç™¾åˆ†æ¯”è¿›è¡Œåˆç†è°ƒæ•´å¹¶åŠ å…¥æ‰°åŠ¨
    """
    judge_percentages = weekly_data['weekly_score_percentage'].values
    eliminated_mask = weekly_data['eliminated_this_week'].values
    
    # åˆå§‹ç²‰ä¸ç™¾åˆ†æ¯”è®¾ä¸ºè¯„å§”ç™¾åˆ†æ¯”ï¼ˆç¡®ä¿æ˜¯numpyæ•°ç»„ï¼‰
    fan_percentages = judge_percentages.copy()
    
    if np.sum(eliminated_mask) > 0:
        # è°ƒæ•´è¢«æ·˜æ±°é€‰æ‰‹çš„ç²‰ä¸ç™¾åˆ†æ¯”ï¼Œä½¿å…¶ç»¼åˆç™¾åˆ†æ¯”æœ€ä½
        eliminated_indices = np.where(eliminated_mask)[0]
        survived_indices = np.where(~eliminated_mask)[0]
        
        for elim_idx in eliminated_indices:
            # è®¡ç®—å¹¸å­˜é€‰æ‰‹çš„æœ€ä½ç»¼åˆç™¾åˆ†æ¯”
            if len(survived_indices) > 0:
                survived_totals = np.array([judge_percentages[i] + fan_percentages[i]
                                         for i in survived_indices])
                min_survived_total = np.min(survived_totals)
                
                # è®¾ç½®æ·˜æ±°é€‰æ‰‹çš„ç²‰ä¸ç™¾åˆ†æ¯”ï¼Œä½¿å…¶ç»¼åˆç™¾åˆ†æ¯”ç•¥ä½äºæœ€å°å¹¸å­˜è€…
                required_fan_percentage = min_survived_total - judge_percentages[elim_idx] - 0.1
                fan_percentages[elim_idx] = max(0, required_fan_percentage)
    
    # åŠ å…¥å°æ‰°åŠ¨ä»¥å½¢æˆåŒºé—´
    fan_percentages = fan_percentages + np.random.normal(0, noise_std, len(fan_percentages))
    fan_percentages = normalize_percentages(fan_percentages)
    
    return fan_percentages

def sample_fan_percentages_monte_carlo(weekly_data, total_votes=TOTAL_VOTES, n_samples=N_SAMPLES, noise_std=NOISE_STD):
    """
    ä½¿ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•é‡‡æ ·ç²‰ä¸æŠ•ç¥¨ç™¾åˆ†æ¯”çš„è§£ç©ºé—´
    å‚è€ƒRankæ¨¡å‹æ€è·¯ï¼šæœ€å°åŒ–ä¸è¯„å§”å·®å¼‚ + çº¦æŸæ¡ä»¶ + éšæœºæ‰°åŠ¨
    """
    weekly_data = weekly_data.reset_index(drop=True)
    n_players = len(weekly_data)
    judge_percentages = weekly_data['weekly_score_percentage'].values
    eliminated_mask = weekly_data['eliminated_this_week'].values
    all_samples = []

    def objective(fan_percentages):
        """ç›®æ ‡å‡½æ•°ï¼šæœ€å°åŒ–ç²‰ä¸ç™¾åˆ†æ¯”ä¸è¯„å§”ç™¾åˆ†æ¯”å·®å¼‚"""
        return np.sum((fan_percentages - judge_percentages) ** 2)
    
    def constraint_sum(fan_percentages):
        """çº¦æŸï¼šç™¾åˆ†æ¯”ä¹‹å’Œä¸º100"""
        return float(np.sum(fan_percentages) - 100)
    
    def constraint_elimination(fan_percentages):
        """çº¦æŸæ¡ä»¶ï¼šè¢«æ·˜æ±°é€‰æ‰‹çš„ç»¼åˆç™¾åˆ†æ¯”å¿…é¡»ä½äºæ‰€æœ‰å¹¸å­˜é€‰æ‰‹"""
        fan_percentages = np.array(fan_percentages)
        total_percentages = judge_percentages + fan_percentages
        
        eliminated_indices = np.where(eliminated_mask)[0]
        survived_indices = np.where(~eliminated_mask)[0]
        
        if len(eliminated_indices) == 0 or len(survived_indices) == 0:
            return 0  # æ— äººè¢«æ·˜æ±°æˆ–æ— äººå¹¸å­˜çš„æƒ…å†µ
        
        eliminated_total = np.array([total_percentages[i] for i in eliminated_indices])
        survived_total = np.array([total_percentages[i] for i in survived_indices])
        
        # æ·˜æ±°é€‰æ‰‹çš„æœ€å¤§ç»¼åˆç™¾åˆ†æ¯”åº”å°äºå¹¸å­˜é€‰æ‰‹çš„æœ€å°ç»¼åˆç™¾åˆ†æ¯”
        max_eliminated = np.max(eliminated_total)
        min_survived = np.min(survived_total)
        
        return float(min_survived - max_eliminated - 0.1)
    
    has_elimination_constraint = np.any(eliminated_mask) and np.any(~eliminated_mask)
    constraints = [{'type': 'eq', 'fun': constraint_sum}]
    if has_elimination_constraint:
        constraints.append({'type': 'ineq', 'fun': constraint_elimination})
    
    for _ in range(n_samples):
        try:
            # åˆå§‹çŒœæµ‹ï¼šè¯„å§”ç™¾åˆ†æ¯” + éšæœºæ‰°åŠ¨
            initial_guess = judge_percentages + np.random.normal(0, noise_std, n_players)
            initial_guess = normalize_percentages(initial_guess)
            
            result = minimize(
                objective,
                initial_guess,
                method='SLSQP',
                bounds=[(0, 100)] * n_players,
                constraints=constraints,
                options={'maxiter': 200, 'ftol': 1e-8}
            )
            
            if result.success:
                sample_result = result.x
                # æ·»åŠ è½»å¾®æ‰°åŠ¨å¢åŠ å¤šæ ·æ€§
                sample_result = sample_result + np.random.normal(0, noise_std * 0.5, n_players)
                sample_result = normalize_percentages(sample_result)
                all_samples.append(sample_result)
            else:
                # å¤±è´¥æ—¶ä½¿ç”¨å¯å‘å¼æ–¹æ³•å¹¶åŠ æ‰°åŠ¨
                fallback = estimate_fan_percentages_heuristic(weekly_data, noise_std=noise_std)
                fallback = fallback + np.random.normal(0, noise_std * 0.5, n_players)
                fallback = normalize_percentages(fallback)
                all_samples.append(fallback)
        except Exception:
            # ä¿åº•ç­–ç•¥ï¼šè¯„å§”ç™¾åˆ†æ¯” + æ›´å¼ºæ‰°åŠ¨
            fallback = judge_percentages + np.random.normal(0, noise_std * 1.5, n_players)
            fallback = normalize_percentages(fallback)
            all_samples.append(fallback)
    
    return np.array(all_samples)

def analyze_percentage_solution_space(samples, weekly_data, total_votes=TOTAL_VOTES):
    """
    åˆ†æç™¾åˆ†æ¯”è§£ç©ºé—´çš„ç»Ÿè®¡ç‰¹æ€§
    """
    analysis_list = []
    n_players = len(weekly_data)
    
    for i, player_name in enumerate(weekly_data['celebrity_name']):
        player_samples = samples[:, i]  # è¯¥é€‰æ‰‹åœ¨æ‰€æœ‰æ ·æœ¬ä¸­çš„ç²‰ä¸ç™¾åˆ†æ¯”
        
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡é‡
        mean_percentage = np.mean(player_samples)
        std_percentage = np.std(player_samples)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´ (95% ç½®ä¿¡æ°´å¹³)
        ci_low = np.percentile(player_samples, CI_LOW)
        ci_high = np.percentile(player_samples, CI_HIGH)
        
        # è®¡ç®—ç™¾åˆ†æ¯”åˆ†å¸ƒçš„ä¼—æ•°
        unique, counts = np.unique(np.round(player_samples, 1), return_counts=True)
        mode_percentage = unique[np.argmax(counts)] if len(unique) > 0 else mean_percentage
        
        # è®¡ç®—ç¥¨æ•°åˆ†å¸ƒ
        sample_votes = []
        for sample_idx in range(samples.shape[0]):
            current_sample_percentages = samples[sample_idx, :]
            current_votes = percentage_to_votes(current_sample_percentages, total_votes)
            sample_votes.append(current_votes[i])
        
        mean_votes = np.mean(sample_votes)
        votes_ci_low = np.percentile(sample_votes, CI_LOW)
        votes_ci_high = np.percentile(sample_votes, CI_HIGH)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´å®½åº¦ï¼ˆåƒç¥¨ä¸ºå•ä½ï¼‰
        ci_width_thousands = (votes_ci_high - votes_ci_low) / 1000
        
        analysis_list.append({
            'celebrity_name': player_name
            , 'judge_percentage': float(weekly_data.iloc[i]['weekly_score_percentage'])
            , 'eliminated_this_week': bool(weekly_data.iloc[i]['eliminated_this_week'])
            , 'mean_fan_percentage': float(mean_percentage)
            , 'std_fan_percentage': float(std_percentage)
            , 'fan_percentage_ci_low': float(ci_low)
            , 'fan_percentage_ci_high': float(ci_high)
            , 'fan_percentage_ci_width': float(ci_high - ci_low)
            , 'mode_fan_percentage': float(mode_percentage)
            , 'mean_fan_votes': float(mean_votes)
            , 'fan_votes_ci_low': float(votes_ci_low)
            , 'fan_votes_ci_high': float(votes_ci_high)
            , 'fan_votes_ci_width': float(ci_width_thousands)
            , 'combined_percentage_mean': float(weekly_data.iloc[i]['weekly_score_percentage'] + mean_percentage)
            , 'certainty_index': float(1 / (std_percentage + 0.1))
        })
    
    return pd.DataFrame(analysis_list)

print("ç¬¬äºŒéƒ¨åˆ†å®Œæˆ! ç™¾åˆ†æ¯”ç¥¨æ•°åˆ†é…å‡½æ•°å·²åˆ›å»ºã€‚")

# %%
# ç¬¬äºŒç‚¹äº”éƒ¨åˆ†ï¼šæ‰°åŠ¨å¼ºåº¦æ ¡å‡†ï¼ˆæµ‹è¯•é˜¶æ®µï¼‰
print("\n=== ç¬¬äºŒç‚¹äº”éƒ¨åˆ†ï¼šæ‰°åŠ¨å¼ºåº¦æ ¡å‡† ===")

def calibrate_noise_std(df, noise_grid, n_samples=80, max_weeks=25, random_state=42):
    """
    æ ¹æ®æ·˜æ±°ä¸€è‡´æ€§ä¸åŒºé—´å®½åº¦ï¼Œæ ¡å‡†æ‰°åŠ¨å¼ºåº¦
    """
    weeks = df[['season', 'week']].drop_duplicates()
    if len(weeks) > max_weeks:
        weeks = weeks.sample(max_weeks, random_state=random_state)
    
    results = []
    
    for noise_std in noise_grid:
        consistency_list = []
        ci_width_list = []
        
        for _, w in weeks.iterrows():
            weekly_data = df[(df['season'] == w['season']) & (df['week'] == w['week'])].copy()
            if len(weekly_data) < 2:
                continue
            
            samples = sample_fan_percentages_monte_carlo(
                weekly_data, n_samples=n_samples, noise_std=noise_std
            )
            analysis = analyze_percentage_solution_space(samples, weekly_data)
            
            eliminated = analysis[analysis['eliminated_this_week'] == True]
            survived = analysis[analysis['eliminated_this_week'] == False]
            if not eliminated.empty and not survived.empty:
                ok = eliminated['combined_percentage_mean'].max() < survived['combined_percentage_mean'].min()
                consistency_list.append(1 if ok else 0)
            
            ci_width_list.append(analysis['fan_percentage_ci_width'].mean())
        
        if ci_width_list:
            results.append({
                'noise_std': noise_std,
                'consistency_rate': np.mean(consistency_list) if consistency_list else np.nan,
                'avg_fan_pct_ci_width': np.mean(ci_width_list),
                'weeks_evaluated': len(ci_width_list)
            })
    
    return pd.DataFrame(results)

# è¿è¡Œæ ¡å‡†ï¼ˆæµ‹è¯•é˜¶æ®µï¼‰
NOISE_GRID = [0.2, 0.4, 0.6, 0.8, 1.0]
calib_summary = calibrate_noise_std(df, NOISE_GRID, n_samples=80, max_weeks=25)
print("\næ‰°åŠ¨å¼ºåº¦æ ¡å‡†ç»“æœ:")
print(calib_summary.round(4).to_string(index=False))

# %%
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŸºäºè§£ç©ºé—´çš„æ‰¹é‡å¤„ç†
print("\n=== ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŸºäºè§£ç©ºé—´çš„æ‰¹é‡å¤„ç† ===")

def process_all_weeks_percentage(df, total_votes=TOTAL_VOTES, n_samples=N_SAMPLES):
    """
    å¤„ç†æ‰€æœ‰å‘¨æ¬¡ï¼Œè¿›è¡Œç™¾åˆ†æ¯”è§£ç©ºé—´åˆ†æ
    """
    all_uncertainty_results = []
    space_characteristics = []
    
    unique_weeks = df[['season', 'week']].drop_duplicates()
    
    print(f"æ€»å…±éœ€è¦å¤„ç† {len(unique_weeks)} ä¸ªå‘¨æ¬¡")
    
    for idx, week_info in enumerate(unique_weeks.iterrows()):
        _, week_info = week_info
        season, week = week_info['season'], week_info['week']
        weekly_data = df[(df['season'] == season) & (df['week'] == week)].copy()
        
        print(f"æ­£åœ¨å¤„ç†èµ›å­£ {season} ç¬¬ {week} å‘¨ ({idx+1}/{len(unique_weeks)})")
        
        if len(weekly_data) < 2:
            print(f"  è·³è¿‡ï¼šé€‰æ‰‹æ•°é‡ä¸è¶³ ({len(weekly_data)})")
            continue
        
        try:
            # 1. é‡‡æ ·è§£ç©ºé—´
            print(f"  å¼€å§‹é‡‡æ ·è§£ç©ºé—´ ({n_samples} ä¸ªæ ·æœ¬)")
            samples = sample_fan_percentages_monte_carlo(weekly_data, total_votes, n_samples)
            print(f"  é‡‡æ ·å®Œæˆï¼Œæ ·æœ¬å½¢çŠ¶: {samples.shape}")
            
            # 2. åˆ†æè§£ç©ºé—´ç‰¹æ€§
            print(f"  å¼€å§‹åˆ†æè§£ç©ºé—´")
            week_analysis = analyze_percentage_solution_space(samples, weekly_data, total_votes)
            week_analysis['season'] = season
            week_analysis['week'] = week
            
            # 3. è®¡ç®—æœ¬å‘¨è§£ç©ºé—´çš„æ•´ä½“ç¡®å®šæ€§
            avg_std = week_analysis['std_fan_percentage'].mean()
            avg_ci_width = (week_analysis['fan_percentage_ci_high'] -
                          week_analysis['fan_percentage_ci_low']).mean()
            
            space_characteristics.append({
                'season': season,
                'week': week,
                'n_players': len(weekly_data),
                'avg_std_fan_percentage': avg_std,
                'avg_ci_width': avg_ci_width,
                'solution_space_compactness': 1 / (avg_std + 0.1),
            })
            
            all_uncertainty_results.append(week_analysis)
            print(f"  å¤„ç†å®Œæˆ")
            
        except Exception as e:
            print(f"å¤„ç†èµ›å­£{season}ç¬¬{week}å‘¨æ—¶å‡ºé”™: {e}")
            continue
    
    if all_uncertainty_results:
        final_uncertainty = pd.concat(all_uncertainty_results, ignore_index=True)
        space_df = pd.DataFrame(space_characteristics)
        return final_uncertainty, space_df
    else:
        return pd.DataFrame(), pd.DataFrame()

# æ‰§è¡Œå¤„ç†
print("å¼€å§‹ç™¾åˆ†æ¯”è§£ç©ºé—´åˆ†æ...")
uncertainty_results, space_stats = process_all_weeks_percentage(df, n_samples=N_SAMPLES)

if not uncertainty_results.empty:
    print(f"è§£ç©ºé—´åˆ†æå®Œæˆ! å…±åˆ†æ {len(space_stats)} ä¸ªå‘¨æ¬¡")
    
    # è®¡ç®—æ•´ä½“ç¡®å®šæ€§æŒ‡æ ‡
    overall_avg_std = space_stats['avg_std_fan_percentage'].mean()
    overall_compactness = space_stats['solution_space_compactness'].mean()
    
    print(f"æ•´ä½“ç™¾åˆ†æ¯”æ ‡å‡†å·®: {overall_avg_std:.3f}")
    print(f"è§£ç©ºé—´ç´§å‡‘æ€§æŒ‡æ•°: {overall_compactness:.3f}")
    
    # ä¿å­˜ç»“æœ
    uncertainty_results.to_csv(OUTPUT_PERCENT_UNCERTAINTY_ANALYSIS, index=False)
    space_stats.to_csv(OUTPUT_PERCENT_SOLUTION_SPACE, index=False)
    print("ç»“æœå·²ä¿å­˜ä¸ºCSVæ–‡ä»¶")
    print(f"- {OUTPUT_PERCENT_UNCERTAINTY_ANALYSIS}")
    print(f"- {OUTPUT_PERCENT_SOLUTION_SPACE}")
else:
    print("è§£ç©ºé—´åˆ†æå¤±è´¥ï¼Œæ— ç»“æœç”Ÿæˆ")

print("\nç¬¬ä¸‰éƒ¨åˆ†å®Œæˆ!")

# %%
# ç¬¬å››éƒ¨åˆ†ï¼šå¯è§†åŒ–åˆ†æ
print("\n=== ç¬¬å››éƒ¨åˆ†ï¼šå¯è§†åŒ–åˆ†æ ===")

def _with_suffix(path, suffix):
    """åœ¨æ–‡ä»¶åæœ«å°¾è¿½åŠ åç¼€ï¼ˆä¿ç•™æ‰©å±•åï¼‰"""
    if '.' in path:
        base, ext = path.rsplit('.', 1)
        return f"{base}{suffix}.{ext}"
    return f"{path}{suffix}"

def plot_percentage_uncertainty_analysis(uncertainty_results, space_stats):
    """
    ç»˜åˆ¶ç™¾åˆ†æ¯”ä¸ç¡®å®šæ€§åˆ†æç»“æœï¼ˆæ‹†åˆ†ä¸º4å¼ å›¾ï¼‰
    """
    if uncertainty_results.empty or space_stats.empty:
        print("æ— å¯è§†åŒ–æ•°æ®ï¼Œè·³è¿‡ç»˜å›¾")
        return
    
    # 1. ç²‰ä¸æŠ•ç¥¨ä¼°è®¡çš„ç½®ä¿¡åŒºé—´ç¤ºä¾‹ï¼ˆç”¨åŒºé—´çº¿æ˜¾ç¤ºèŒƒå›´ï¼‰
    try:
        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
        sample_week_data = uncertainty_results[
            (uncertainty_results['season'] == 3) & 
            (uncertainty_results['week'] == 1.0)
        ].head(8)
        
        if not sample_week_data.empty:
            y_pos = range(len(sample_week_data))
            for i, (_, row) in enumerate(sample_week_data.iterrows()):
                ax.hlines(y=i, xmin=row['fan_votes_ci_low'], xmax=row['fan_votes_ci_high'],
                          color='steelblue', alpha=0.8, linewidth=3)
                ax.plot(row['mean_fan_votes'], i, 'o', color='black', markersize=4)
            ax.set_yticks(list(y_pos))
            ax.set_yticklabels(sample_week_data['celebrity_name'], fontsize=8)
            ax.set_xlabel('ç²‰ä¸æŠ•ç¥¨æ•°ä¼°è®¡ï¼ˆåŒºé—´ï¼‰')
            ax.set_title('ç²‰ä¸æŠ•ç¥¨ä¼°è®¡çš„ç½®ä¿¡åŒºé—´ï¼ˆèµ›å­£3ç¬¬1å‘¨ï¼‰')
            ax.grid(True, axis='x')
        else:
            ax.set_title('ç²‰ä¸æŠ•ç¥¨ä¼°è®¡çš„ç½®ä¿¡åŒºé—´ï¼ˆèµ›å­£3ç¬¬1å‘¨ï¼‰')
            ax.text(0.5, 0.5, 'æ— å¯ç”¨æ•°æ®', ha='center', va='center', transform=ax.transAxes)
        
        fig.tight_layout()
        path1 = _with_suffix(OUTPUT_PERCENT_VISUALIZATION, '_1')
        fig.savefig(path1, dpi=300, bbox_inches='tight')
        plt.show()
        print(f"å›¾1å·²ä¿å­˜: {path1}")
    except Exception as e:
        print(f"ç»˜åˆ¶å›¾1æ—¶å‡ºé”™: {e}")
    
    # 2. è¯„å§”ç™¾åˆ†æ¯” vs ç²‰ä¸ç™¾åˆ†æ¯”æ•£ç‚¹å›¾
    try:
        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
        sample_data = uncertainty_results.head(100)
        ax.scatter(sample_data['judge_percentage'], sample_data['mean_fan_percentage'],
                   alpha=0.6, c=sample_data['eliminated_this_week'].map({True: 'red', False: 'blue'}))
        ax.set_xlabel('è¯„å§”ç™¾åˆ†æ¯”')
        ax.set_ylabel('ç²‰ä¸ç™¾åˆ†æ¯”ä¼°è®¡')
        ax.set_title('è¯„å§”ç™¾åˆ†æ¯” vs ç²‰ä¸ç™¾åˆ†æ¯”ä¼°è®¡')
        ax.grid(True, alpha=0.3)
        lims = [0, max(ax.get_xlim()[1], ax.get_ylim()[1])]
        ax.plot(lims, lims, 'k--', alpha=0.3)
        ax.set_xlim(lims)
        ax.set_ylim(lims)
        
        fig.tight_layout()
        path2 = _with_suffix(OUTPUT_PERCENT_VISUALIZATION, '_2')
        fig.savefig(path2, dpi=300, bbox_inches='tight')
        plt.show()
        print(f"å›¾2å·²ä¿å­˜: {path2}")
    except Exception as e:
        print(f"ç»˜åˆ¶å›¾2æ—¶å‡ºé”™: {e}")
    
    # 3. ç»¼åˆç™¾åˆ†æ¯”åˆ†å¸ƒ
    try:
        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
        combined_data = uncertainty_results['combined_percentage_mean']
        ax.hist(combined_data, bins=30, alpha=0.7, color='green')
        ax.axvline(combined_data.mean(), color='red', linestyle='--',
                   label=f'å¹³å‡å€¼: {combined_data.mean():.1f}%')
        ax.set_xlabel('ç»¼åˆç™¾åˆ†æ¯”ï¼ˆè¯„å§”+ç²‰ä¸ï¼‰')
        ax.set_ylabel('é¢‘æ•°')
        ax.set_title('ç»¼åˆç™¾åˆ†æ¯”åˆ†å¸ƒ')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        fig.tight_layout()
        path3 = _with_suffix(OUTPUT_PERCENT_VISUALIZATION, '_3')
        fig.savefig(path3, dpi=300, bbox_inches='tight')
        plt.show()
        print(f"å›¾3å·²ä¿å­˜: {path3}")
    except Exception as e:
        print(f"ç»˜åˆ¶å›¾3æ—¶å‡ºé”™: {e}")
    
    # 4. å„èµ›å­£å¹³å‡ç¡®å®šæ€§æ¯”è¾ƒ
    try:
        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
        season_certainty = uncertainty_results.groupby('season')['certainty_index'].mean()
        seasons = season_certainty.index.astype(int)
        values = season_certainty.values
        ax.bar(seasons, values, color='lightcoral', alpha=0.7)
        ax.set_xlabel('èµ›å­£')
        ax.set_ylabel('å¹³å‡ç¡®å®šæ€§æŒ‡æ•°')
        ax.set_title('å„èµ›å­£ä¼°è®¡ç¡®å®šæ€§æ¯”è¾ƒ')
        ax.grid(True, alpha=0.3)
        
        fig.tight_layout()
        path4 = _with_suffix(OUTPUT_PERCENT_VISUALIZATION, '_4')
        fig.savefig(path4, dpi=300, bbox_inches='tight')
        plt.show()
        print(f"å›¾4å·²ä¿å­˜: {path4}")
    except Exception as e:
        print(f"ç»˜åˆ¶å›¾4æ—¶å‡ºé”™: {e}")

# æ‰§è¡Œå¯è§†åŒ–
if not uncertainty_results.empty and not space_stats.empty:
    print("å¼€å§‹ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨...")
    plot_percentage_uncertainty_analysis(uncertainty_results, space_stats)
else:
    print("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå¯è§†åŒ–")

print("\nç¬¬å››éƒ¨åˆ†å®Œæˆ!")

# %%
# ç¬¬å››ç‚¹äº”éƒ¨åˆ†ï¼šæ–‡æœ¬æ–¹å¼æŸ¥çœ‹é¢„æµ‹åŒºé—´
print("\n=== ç¬¬å››ç‚¹äº”éƒ¨åˆ†ï¼šæ–‡æœ¬é¢„æµ‹åŒºé—´å±•ç¤º ===")

def show_text_prediction_intervals(uncertainty_results, season=3, week=1.0, top_n=10):
    """
    ç”¨æ–‡æœ¬æ–¹å¼å±•ç¤ºæŒ‡å®šå‘¨æ¬¡çš„é¢„æµ‹åŒºé—´
    """
    week_data = uncertainty_results[
        (uncertainty_results['season'] == season) & 
        (uncertainty_results['week'] == week)
    ].copy()
    
    if week_data.empty:
        print(f"æœªæ‰¾åˆ°èµ›å­£{season}ç¬¬{week}å‘¨çš„æ•°æ®")
        return
    
    # é€‰å–åŒºé—´å®½åº¦æœ€å¤§çš„å‰Nä½ï¼Œæ–¹ä¾¿è§‚å¯ŸèŒƒå›´
    week_data['ci_width'] = week_data['fan_votes_ci_high'] - week_data['fan_votes_ci_low']
    week_data = week_data.sort_values('ci_width', ascending=False).head(top_n)
    
    display_cols = [
        'celebrity_name', 'judge_percentage', 'mean_fan_percentage',
        'fan_percentage_ci_low', 'fan_percentage_ci_high',
        'mean_fan_votes', 'fan_votes_ci_low', 'fan_votes_ci_high',
        'eliminated_this_week'
    ]
    print(f"\nèµ›å­£{season}ç¬¬{week}å‘¨ - é¢„æµ‹åŒºé—´ï¼ˆTop {top_n}ï¼‰:")
    print(week_data[display_cols].round(2).to_string(index=False))

def show_single_player_interval(uncertainty_results, season=3, week=1.0):
    """
    éšæœºæŒ‘ä¸€åé€‰æ‰‹ï¼Œå±•ç¤ºå…¶é¢„æµ‹åŒºé—´
    """
    week_data = uncertainty_results[
        (uncertainty_results['season'] == season) & 
        (uncertainty_results['week'] == week)
    ].copy()
    if week_data.empty:
        print(f"æœªæ‰¾åˆ°èµ›å­£{season}ç¬¬{week}å‘¨çš„æ•°æ®")
        return
    
    sample_row = week_data.sample(1, random_state=42).iloc[0]
    print("\néšæœºé€‰æ‰‹é¢„æµ‹åŒºé—´:")
    print(f"é€‰æ‰‹: {sample_row['celebrity_name']}")
    print(f"è¯„å§”ç™¾åˆ†æ¯”: {sample_row['judge_percentage']:.2f}%")
    print(f"ç²‰ä¸ç™¾åˆ†æ¯”å‡å€¼: {sample_row['mean_fan_percentage']:.2f}%")
    print(f"ç²‰ä¸ç™¾åˆ†æ¯”åŒºé—´: [{sample_row['fan_percentage_ci_low']:.2f}%, {sample_row['fan_percentage_ci_high']:.2f}%]")
    print(f"ç²‰ä¸ç¥¨æ•°å‡å€¼: {sample_row['mean_fan_votes']:.0f}")
    print(f"ç²‰ä¸ç¥¨æ•°åŒºé—´: [{sample_row['fan_votes_ci_low']:.0f}, {sample_row['fan_votes_ci_high']:.0f}]")
    print(f"æ˜¯å¦è¢«æ·˜æ±°: {sample_row['eliminated_this_week']}")

# æ‰§è¡Œæ–‡æœ¬å±•ç¤º
if not uncertainty_results.empty:
    show_text_prediction_intervals(uncertainty_results, season=3, week=1.0, top_n=10)
    show_single_player_interval(uncertainty_results, season=3, week=1.0)
else:
    print("æ— å¯ç”¨çš„ä¸ç¡®å®šæ€§ç»“æœ")

# %%
# ç¬¬äº”éƒ¨åˆ†ï¼šç”Ÿæˆé¢„æµ‹ç¥¨æ•°æ–‡ä»¶
print("\n=== ç¬¬äº”éƒ¨åˆ†ï¼šç”Ÿæˆé¢„æµ‹ç¥¨æ•°æ–‡ä»¶ ===")

def generate_predicted_votes_for_merge(uncertainty_results):
    """
    ä»ä¸ç¡®å®šæ€§åˆ†æç»“æœä¸­æå–é¢„æµ‹ç¥¨æ•°ï¼Œç”Ÿæˆä¾›åˆå¹¶ä½¿ç”¨çš„æ–‡ä»¶
    """
    if uncertainty_results.empty:
        print("æ— ä¸ç¡®å®šæ€§åˆ†æç»“æœï¼Œæ— æ³•ç”Ÿæˆé¢„æµ‹ç¥¨æ•°æ–‡ä»¶")
        return pd.DataFrame()
    
    # é€‰æ‹©éœ€è¦çš„åˆ—
    predicted_votes = uncertainty_results[[
        'season', 'week', 'celebrity_name', 'mean_fan_votes', 'mean_fan_percentage'
    ]].copy()
    
    # é‡å‘½ååˆ—
    predicted_votes = predicted_votes.rename(columns={
        'mean_fan_votes': 'predicted_fan_votes',
        'mean_fan_percentage': 'predicted_fan_percentage'
    })
    
    # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
    predicted_votes['season'] = pd.to_numeric(predicted_votes['season'], errors='coerce')
    predicted_votes['week'] = pd.to_numeric(predicted_votes['week'], errors='coerce')
    
    # ä¿å­˜æ–‡ä»¶
    predicted_votes.to_csv(OUTPUT_PERCENT_PREDICTED_VOTES, index=False)
    
    print(f"é¢„æµ‹ç¥¨æ•°æ–‡ä»¶ç”Ÿæˆå®Œæˆ!")
    print(f"è®°å½•æ•°é‡: {len(predicted_votes)}")
    print(f"èµ›å­£èŒƒå›´: {predicted_votes['season'].min()} - {predicted_votes['season'].max()}")
    print(f"å‘¨æ¬¡èŒƒå›´: {predicted_votes['week'].min()} - {predicted_votes['week'].max()}")
    print(f"é€‰æ‰‹æ•°é‡: {predicted_votes['celebrity_name'].nunique()}")
    print(f"æ–‡ä»¶å·²ä¿å­˜ä¸º: {OUTPUT_PERCENT_PREDICTED_VOTES}")
    
    return predicted_votes

# ç”Ÿæˆé¢„æµ‹ç¥¨æ•°æ–‡ä»¶
if not uncertainty_results.empty:
    print("æ­£åœ¨ç”Ÿæˆé¢„æµ‹ç¥¨æ•°æ–‡ä»¶...")
    predicted_votes_df = generate_predicted_votes_for_merge(uncertainty_results)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\né¢„æµ‹ç¥¨æ•°ç»Ÿè®¡:")
    print(f"å¹³å‡é¢„æµ‹ç¥¨æ•°: {predicted_votes_df['predicted_fan_votes'].mean():.0f}")
    print(f"é¢„æµ‹ç¥¨æ•°èŒƒå›´: {predicted_votes_df['predicted_fan_votes'].min():.0f} - {predicted_votes_df['predicted_fan_votes'].max():.0f}")
    print(f"å¹³å‡ç²‰ä¸ç™¾åˆ†æ¯”: {predicted_votes_df['predicted_fan_percentage'].mean():.2f}%")
else:
    print("æœªæ‰¾åˆ°å¯ç”¨çš„ä¸ç¡®å®šæ€§åˆ†æç»“æœ")

print("\nç¬¬äº”éƒ¨åˆ†å®Œæˆ!")

# %%
# ç¬¬å…­éƒ¨åˆ†ï¼šæ¨¡å‹éªŒè¯å’Œæ€»ç»“
print("\n=== ç¬¬å…­éƒ¨åˆ†ï¼šæ¨¡å‹éªŒè¯å’Œæ€»ç»“ ===")

def validate_percentage_model(uncertainty_results):
    """
    éªŒè¯ç™¾åˆ†æ¯”æ¨¡å‹çš„åˆç†æ€§
    """
    if uncertainty_results.empty:
        print("æ— æ•°æ®å¯éªŒè¯")
        return
    
    print("æ¨¡å‹éªŒè¯ç»“æœ:")
    
    # 1. æ£€æŸ¥æ·˜æ±°é€‰æ‰‹çš„ç»¼åˆç™¾åˆ†æ¯”æ˜¯å¦ä½äºå¹¸å­˜é€‰æ‰‹
    eliminated_data = uncertainty_results[uncertainty_results['eliminated_this_week'] == True]
    survived_data = uncertainty_results[uncertainty_results['eliminated_this_week'] == False]
    
    if not eliminated_data.empty and not survived_data.empty:
        avg_eliminated_combined = eliminated_data['combined_percentage_mean'].mean()
        avg_survived_combined = survived_data['combined_percentage_mean'].mean()
        
        print(f"æ·˜æ±°é€‰æ‰‹å¹³å‡ç»¼åˆç™¾åˆ†æ¯”: {avg_eliminated_combined:.2f}%")
        print(f"å¹¸å­˜é€‰æ‰‹å¹³å‡ç»¼åˆç™¾åˆ†æ¯”: {avg_survived_combined:.2f}%")
        
        if avg_eliminated_combined < avg_survived_combined:
            print("âœ“ æ·˜æ±°è§„åˆ™éªŒè¯é€šè¿‡: æ·˜æ±°é€‰æ‰‹ç»¼åˆç™¾åˆ†æ¯”ä½äºå¹¸å­˜é€‰æ‰‹")
        else:
            print("âœ— æ·˜æ±°è§„åˆ™éªŒè¯å¤±è´¥: æ·˜æ±°é€‰æ‰‹ç»¼åˆç™¾åˆ†æ¯”å¼‚å¸¸")
    
    # 2. æ£€æŸ¥ç™¾åˆ†æ¯”åˆ†å¸ƒåˆç†æ€§
    fan_percentages = uncertainty_results['mean_fan_percentage']
    print(f"ç²‰ä¸ç™¾åˆ†æ¯”åˆ†å¸ƒ: {fan_percentages.min():.1f}% - {fan_percentages.max():.1f}%")
    print(f"ç²‰ä¸ç™¾åˆ†æ¯”æ ‡å‡†å·®: {fan_percentages.std():.2f}%")
    
    # 3. æ£€æŸ¥ç¥¨æ•°åˆ†å¸ƒåˆç†æ€§
    fan_votes = uncertainty_results['mean_fan_votes']
    total_predicted_votes = fan_votes.sum()
    expected_total = TOTAL_VOTES * len(uncertainty_results[['season', 'week']].drop_duplicates())
    
    print(f"é¢„æµ‹æ€»ç¥¨æ•°: {total_predicted_votes:,.0f}")
    print(f"é¢„æœŸæ€»ç¥¨æ•°: {expected_total:,.0f}")
    print(f"ç¥¨æ•°åå·®: {(total_predicted_votes - expected_total) / expected_total * 100:.2f}%")
    
    return True

# æ‰§è¡Œæ¨¡å‹éªŒè¯
if not uncertainty_results.empty:
    validation_result = validate_percentage_model(uncertainty_results)
    
    print(f"\n=== ç™¾åˆ†æ¯”æœºåˆ¶æ¨¡å‹æ€»ç»“ ===")
    print("æ¨¡å‹ç‰¹ç‚¹:")
    print("1. åŸºäºç™¾åˆ†æ¯”æœºåˆ¶è®¾è®¡ï¼Œç¬¦åˆèµ›å­£3-27çš„å®é™…è§„åˆ™")
    print("2. ä½¿ç”¨ç»„åˆæƒé‡ç¡®ä¿ç¥¨æ•°åˆç†åˆ†é…")
    print("3. è’™ç‰¹å¡æ´›é‡‡æ ·è€ƒè™‘æ·˜æ±°çº¦æŸå’Œç™¾åˆ†æ¯”æ±‚å’Œçº¦æŸ")
    print("4. è¾“å‡ºåŒ…å«ç½®ä¿¡åŒºé—´å’Œä¸ç¡®å®šæ€§æŒ‡æ ‡")
    
    print(f"\nåˆ†æèŒƒå›´:")
    print(f"- èµ›å­£æ•°é‡: {uncertainty_results['season'].nunique()}")
    print(f"- å‘¨æ¬¡æ•°é‡: {uncertainty_results[['season', 'week']].drop_duplicates().shape[0]}")
    print(f"- é€‰æ‰‹è®°å½•: {len(uncertainty_results)}")
    
    print(f"\nç”Ÿæˆæ–‡ä»¶:")
    print("1. percentage_fan_vote_uncertainty_analysis.csv - è¯¦ç»†ä¸ç¡®å®šæ€§åˆ†æ")
    print("2. percentage_solution_space_characteristics.csv - è§£ç©ºé—´ç‰¹æ€§")
    print("3. percentage_predicted_fan_votes.csv - é¢„æµ‹ç¥¨æ•°æ–‡ä»¶")
    print("4. percentage_uncertainty_analysis.png - å¯è§†åŒ–å›¾è¡¨")
else:
    print("æ— æ³•è¿›è¡Œæ¨¡å‹éªŒè¯")

print("\næ‰€æœ‰ä»£ç æ‰§è¡Œå®Œæˆ! ğŸ‰")

# %%



